# OLLAMA with Open WebUI and LINE Chatbot
# Customize with NGINX (Reverse-proxy) and NGROK (Local Tunnelling)
# Created and Modified by PEACE & CHATGPT @ 17022025

services:
  ollama:
    image: ollama/ollama
    container_name: ollama
    restart: unless-stopped
    env_file:
      - .env
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    # If your GPU is NVIDIA, UNCOMMENT Line 17-27. 
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: all
    #           capabilities: [gpu]
    # environment:
    #   - OLLAMA_ENABLE_CUDA=1
    #   - NVIDIA_VISIBLE_DEVICES=all
    #   - NVIDIA_DRIVER_CAPABILITIES=compute,utility

  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: open-webui
    restart: unless-stopped
    env_file:
      - .env
    ports:
      - "8080:8080"
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434
    depends_on:
      - ollama
    extra_hosts:
      - "host.docker.internal:host-gateway"
    volumes:
      - open-webui-local:/app/backend/data

  line-chat:
    build: ./line-chat
    container_name: line-chat
    restart: unless-stopped
    env_file:
      - .env
    environment:
      - CHANNEL_ACCESS_TOKEN=${CHANNEL_ACCESS_TOKEN}
      - CHANNEL_SECRET=${CHANNEL_SECRET}
      - OLLAMA_API_URL=http://ollama:11434/api/generate
    ports:
      - "8000:8000"
    depends_on:
      - ollama

  ngrok:
    image: ngrok/ngrok:latest
    container_name: ngrok
    restart: always
    command: ["http", "reverse-proxy:80"]
    environment:
      - NGROK_AUTHTOKEN=${NGROK_AUTHTOKEN}
    depends_on:
      - reverse-proxy

  reverse-proxy:
    image: nginx:latest
    container_name: reverse-proxy
    restart: always
    ports:
      - "80:80"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf:ro
    depends_on:
      - ollama
      - open-webui
      - line-chat

volumes:
  ollama_data:
    driver: local
  open-webui-local:
    driver: local
